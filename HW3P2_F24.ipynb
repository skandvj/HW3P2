{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA9qZoIDcx-h"
      },
      "outputs": [],
      "source": [
        "# %pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xnrq5qYJnBuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Uninstall the specific packages\n",
        "# !pip uninstall torchsummaryx -y\n",
        "# !pip uninstall wandb -y\n",
        "# !pip uninstall python-Levenshtein -y\n",
        "# !pip uninstall wget -y\n",
        "\n",
        "# # If ctcdecode was installed, uninstall it as well\n",
        "# !pip uninstall ctcdecode -y\n",
        "\n",
        "# # Remove the ctcdecode directory if it was cloned\n",
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# if os.path.isdir(\"ctcdecode\"):\n",
        "#     shutil.rmtree(\"ctcdecode\")\n"
      ],
      "metadata": {
        "id": "Uh87Q3RonBoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "b22c330a-000e-4576-aac6-23d130070688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummaryx==1.1.0 in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "fatal: destination path 'ctcdecode' already exists and is not an empty directory.\n",
            "/content/ctcdecode\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/extension.py:139: UserWarning: Unknown Extension options: 'package', 'with_cuda'\n",
            "  warnings.warn(msg)\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing ctcdecode.egg-info/PKG-INFO\n",
            "writing dependency_links to ctcdecode.egg-info/dependency_links.txt\n",
            "writing top-level names to ctcdecode.egg-info/top_level.txt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "reading manifest file 'ctcdecode.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'ctcdecode.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "copying ctcdecode/__init__.py -> build/lib.linux-x86_64-cpython-310/ctcdecode\n",
            "running build_ext\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/ctcdecode\n",
            "creating build/bdist.linux-x86_64/egg/ctcdecode/_ext\n",
            "copying build/lib.linux-x86_64-cpython-310/ctcdecode/_ext/ctc_decode.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/ctcdecode/_ext\n",
            "copying build/lib.linux-x86_64-cpython-310/ctcdecode/__init__.py -> build/bdist.linux-x86_64/egg/ctcdecode\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ctcdecode/__init__.py to __init__.cpython-310.pyc\n",
            "creating stub loader for ctcdecode/_ext/ctc_decode.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ctcdecode/_ext/ctc_decode.py to ctc_decode.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ctcdecode.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ctcdecode.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ctcdecode.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ctcdecode.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "ctcdecode._ext.__pycache__.ctc_decode.cpython-310: module references __file__\n",
            "creating 'dist/ctcdecode-1.0.3-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing ctcdecode-1.0.3-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/ctcdecode-1.0.3-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/ctcdecode-1.0.3-py3.10-linux-x86_64.egg\n",
            "Extracting ctcdecode-1.0.3-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding ctcdecode 1.0.3 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/ctcdecode-1.0.3-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for ctcdecode==1.0.3\n",
            "Finished processing dependencies for ctcdecode==1.0.3\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchsummaryx==1.1.0\n",
        "# !pip install wandb --quiet\n",
        "# !pip install python-Levenshtein -q\n",
        "# !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "# !pip install wget -q\n",
        "# %cd ctcdecode\n",
        "# !pip install . -q\n",
        "# !python setup.py install\n",
        "# %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmLNlXPkRvXV",
        "outputId": "5682dae5-8e95-4799-c9ed-056da4b97e89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "#!pip install torchaudio\n",
        "# !pip install pytorch-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvFLKO_E-rx1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip uninstall ctcdecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "f9602b22-1323-4e93-c7cc-6577dc327586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torchnlp.nn import LockedDropout\n",
        "from torchaudio.transforms import FrequencyMasking, TimeMasking\n",
        "from torchinfo import summary\n",
        "import torchaudio\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUelfGhom1m",
        "outputId": "4b5f701d-2bec-4a57-a434-9170c5f1b58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8 -q\n",
        "# !mkdir /root/.kaggle\n",
        "\n",
        "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "#     f.write('{\"username\":\"skandv\",\"key\":\"0cafa647bf22cf57326d60803b03911c\"}') # TODO: Put your kaggle username & key here\n",
        "\n",
        "# !chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGtIDZtYr6pG",
        "outputId": "7076370a-54b6-4ad1-8ee3-6aa629c39897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjBwfXeoq4B",
        "outputId": "58515ebb-aac1-4473-f33b-0c1b1cb6a74f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11-785-hw3p2-f24.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "# !kaggle competitions download -c 11-785-hw3p2-f24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ruxWP60LCQA",
        "outputId": "74784822-953b-4aea-b9f0-b45bb6b2af64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace 11785-f24-hw3p2/dev-clean/mfcc/1272-128104-0000.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: 11785-f24-hw3p2  11-785-hw3p2-f24.zip  checkpoint  ctcdecode  sample_data  wandb\n"
          ]
        }
      ],
      "source": [
        "#  '''\n",
        "#  This will take a couple minutes, but you should see at least the following:\n",
        "#  11-785-f24-hw3p2  ctcdecode  hw3p2asr-f24.zip  sample_data\n",
        "#  '''\n",
        "#  !unzip -q 11-785-hw3p2-f24.zip\n",
        "# #  !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cp-716IMZRd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQXHFdQsxQ23"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]\n",
        "# PHONEMES = sorted(PHONEMES)\n",
        "# LABELS = sorted(LABELS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN2kcxwXLLBb",
        "outputId": "f36c69bc-bc95-4562-90cf-8ff252c2b872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[SIL]', 'NG', 'F', 'M', 'AE', 'R', 'UW', 'N', 'IY', 'AW', 'V', 'UH', 'OW', 'AA', 'ER', 'HH', 'Z', 'K', 'CH', 'W', 'EY', 'ZH', 'T', 'EH', 'Y', 'AH', 'B', 'P', 'TH', 'DH', 'AO', 'G', 'L', 'JH', 'OY', 'SH', 'D', 'AY', 'S', 'IH']\n",
            "41\n",
            "[' ', '-', 'G', 'f', 'm', '@', 'r', 'u', 'n', 'i', 'W', 'v', 'U', 'o', 'a', 'R', 'h', 'z', 'k', 'C', 'w', 'e', 'Z', 't', 'E', 'y', 'A', 'b', 'p', 'T', 'D', 'c', 'g', 'l', 'j', 'O', 'S', 'd', 'Y', 's', 'I']\n",
            "41\n"
          ]
        }
      ],
      "source": [
        "# Printing the PHONEMES list to inspect its content and ensure it contains all expected phonemes.\n",
        "# This is a good way to verify that the data was loaded or defined correctly.\n",
        "print(PHONEMES)\n",
        "\n",
        "# Printing the length of the PHONEMES list to ensure the number of phonemes matches expectations.\n",
        "# This step helps confirm that there are no missing or duplicate entries in the phoneme list.\n",
        "print(len(PHONEMES))\n",
        "\n",
        "# Printing the LABELS list to inspect its content and validate that it aligns with the expected label mappings.\n",
        "# This can serve as a sanity check to ensure that labels correspond accurately to the phonemes.\n",
        "print(LABELS)\n",
        "\n",
        "# Printing the length of the LABELS list to ensure it has the correct number of entries, ideally matching the number of phonemes.\n",
        "# If the length does not match, it may indicate an issue with the mapping or data preprocessing.\n",
        "print(len(LABELS))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F5vLj6s0t7T",
        "outputId": "3cfb3a6b-eefc-4018-b603-674175a5ba42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MFCC shape: (1404, 28)\n",
            "Transcript shape: (147,)\n",
            "MFCC shape: (1590, 28)\n",
            "Transcript shape: (196,)\n",
            "MFCC shape: (1390, 28)\n",
            "Transcript shape: (179,)\n",
            "MFCC shape: (1467, 28)\n",
            "Transcript shape: (183,)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define root directory for dataset\n",
        "dataset_root = '/content/11785-f24-hw3p2'\n",
        "train_partition = 'train-clean-100'\n",
        "\n",
        "# Define directories for MFCC and transcript files\n",
        "mfcc_directory = os.path.join(dataset_root, train_partition, \"mfcc\")\n",
        "transcript_directory = os.path.join(dataset_root, train_partition, \"transcript\")\n",
        "\n",
        "# List all files in the directories in sorted order\n",
        "mfcc_file_list = sorted(os.listdir(mfcc_directory))\n",
        "transcript_file_list = sorted(os.listdir(transcript_directory))\n",
        "\n",
        "# Initialize a list to store MFCC lengths\n",
        "mfcc_time_steps = []\n",
        "\n",
        "# Iterate through the MFCC and transcript files\n",
        "for index in range(len(mfcc_file_list)):\n",
        "    # Construct full paths for the current MFCC and transcript files\n",
        "    mfcc_file_path = os.path.join(mfcc_directory, mfcc_file_list[index])\n",
        "    transcript_file_path = os.path.join(transcript_directory, transcript_file_list[index])\n",
        "\n",
        "    # Load the MFCC data\n",
        "    mfcc_data = np.load(mfcc_file_path)\n",
        "    mfcc_time_steps.append(mfcc_data.shape[0])  # Store the number of time steps\n",
        "\n",
        "    # Load the transcript data\n",
        "    transcript_data = np.load(transcript_file_path)\n",
        "\n",
        "    # Print shapes of the MFCC and transcript data\n",
        "    print(f\"MFCC shape: {mfcc_data.shape}\")\n",
        "    print(f\"Transcript shape: {transcript_data.shape}\")\n",
        "\n",
        "    # Break after processing the first 4 files as a sample\n",
        "    if index == 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, root, partition=\"train-clean-100\"):\n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = os.path.join(root, partition, \"mfcc\")\n",
        "        self.transcript_dir = os.path.join(root, partition, \"transcript\")\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        # TODO: WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        # TODO: HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "        self.phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(PHONEMES)}\n",
        "        self.index_to_phoneme = {idx: phoneme for idx, phoneme in enumerate(PHONEMES)}\n",
        "\n",
        "        # TODO: CREATE AN ARRAY OF ALL FEATURES AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        for i in range(self.length):\n",
        "            mfcc_path = os.path.join(self.mfcc_dir, self.mfcc_files[i])\n",
        "            transcript_path = os.path.join(self.transcript_dir, self.transcript_files[i])\n",
        "\n",
        "            mfcc = np.load(mfcc_path)\n",
        "            mfcc = mfcc - np.mean(mfcc, axis=0, keepdims=True)\n",
        "            mfcc = mfcc / np.std(mfcc, axis=0, keepdims=True)\n",
        "\n",
        "            transcript = np.load(transcript_path)[1:-1]\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append([self.phoneme_to_index[i] for i in transcript])\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        mfcc = self.mfccs[ind]\n",
        "        transcript = self.transcripts[ind]\n",
        "        return mfcc, transcript\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish.\n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lengths of features,\n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [item[0] for item in batch]\n",
        "        # batch of output phonemes\n",
        "        batch_transcript = [torch.tensor(item[1], dtype=torch.long) for item in batch]\n",
        "\n",
        "        lengths_mfcc = [mfcc.shape[0] for mfcc in batch_mfcc]\n",
        "        lengths_transcript = [len(transcript) for transcript in batch_transcript]\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(\n",
        "            [torch.tensor(mfcc, dtype=torch.float32) for mfcc in batch_mfcc],\n",
        "            batch_first=True,\n",
        "            padding_value=0.0,\n",
        "        )\n",
        "\n",
        "        # Use a custom padding value (-1) for the transcripts\n",
        "        batch_transcript_pad = pad_sequence(\n",
        "            batch_transcript, batch_first=True, padding_value=-1\n",
        "        )\n",
        "\n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return (\n",
        "            batch_mfcc_pad,\n",
        "            batch_transcript_pad,\n",
        "            torch.tensor(lengths_mfcc),\n",
        "            torch.tensor(lengths_transcript),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test DataLoader\n",
        "class TestAudioDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, partition=\"test-clean\"):\n",
        "        '''\n",
        "        Initializes the test dataset.\n",
        "\n",
        "        Args:\n",
        "            root (str): Root directory containing the dataset.\n",
        "            subset (str): Specific partition of the dataset (e.g., \"test-clean\").\n",
        "        '''\n",
        "\n",
        "        # Directory containing MFCC feature files\n",
        "        self.feature_dir = os.path.join(root, partition, \"mfcc\")\n",
        "        self.feature_files = sorted(os.listdir(self.feature_dir))\n",
        "\n",
        "        # Total number of samples in the dataset\n",
        "        self.dataset_size = len(self.feature_files)\n",
        "\n",
        "        # Preload and normalize all MFCC features\n",
        "        self.normalized_features = []\n",
        "        for idx in range(self.dataset_size):\n",
        "            feature_path = os.path.join(self.feature_dir, self.feature_files[idx])\n",
        "            mfcc_data = np.load(feature_path)\n",
        "\n",
        "            # Normalize MFCC data\n",
        "            mfcc_normalized = (mfcc_data - np.mean(mfcc_data, axis=0, keepdims=True)) / np.std(\n",
        "                mfcc_data, axis=0, keepdims=True\n",
        "            )\n",
        "            self.normalized_features.append(mfcc_normalized)\n",
        "\n",
        "    def __len__(self):\n",
        "        ''' Returns the total number of samples in the dataset. '''\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        Returns the normalized MFCC coefficients for the given index.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the desired sample.\n",
        "\n",
        "        Returns:\n",
        "            numpy array: Normalized MFCC features for the sample.\n",
        "        '''\n",
        "        return self.normalized_features[index]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        '''\n",
        "        Collate function to process a batch of test data.\n",
        "\n",
        "        Args:\n",
        "            batch: List of MFCC features.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Padded MFCC features and their original lengths.\n",
        "        '''\n",
        "\n",
        "        # Convert batch items to tensors and determine their original lengths\n",
        "        feature_tensors = [torch.tensor(features, dtype=torch.float32) for features in batch]\n",
        "        feature_lengths = [features.shape[0] for features in feature_tensors]\n",
        "\n",
        "        # Pad feature tensors to create a uniform batch\n",
        "        padded_features = pad_sequence(\n",
        "            feature_tensors,\n",
        "            batch_first=True,\n",
        "            padding_value=0.0  # Silence padding\n",
        "        )\n",
        "\n",
        "        # Return padded features and their lengths\n",
        "        return padded_features, torch.tensor(feature_lengths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Config - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "root = \"/content/11785-f24-hw3p2\"\n",
        "\n",
        "# Feel free to add more items here\n",
        "config = {\n",
        "            \"batch_size\" : 21,\n",
        "            \"epochs\"     : 35,\n",
        "            \"beam_width\" :  4,\n",
        "            \"lr\"         : 2e-3\n",
        "}\n",
        "\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "transforms = [] # set of tranformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "80ba5230-3258-42d2-9732-37fbc818cf22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1067"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "# get me RAMMM!!!!\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "919f6d83-97d8-40e3-d206-abc781b1298c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  21\n",
            "Training samples: 28539, Training batches: 1359\n",
            "Validation samples: 2703, Validation batches: 129\n",
            "Test samples: 2620, Test batches: 125\n"
          ]
        }
      ],
      "source": [
        "# Initialize Dataset Objects\n",
        "# data_root: Path to the dataset directory\n",
        "# subset: Specific partition of the dataset (e.g., \"train-clean-100\", \"dev-clean\", \"test-clean\")\n",
        "\n",
        "train_dataset = AudioDataset(root, partition=\"train-clean-100\")  # Updated variable names\n",
        "validation_dataset = AudioDataset(root, partition=\"dev-clean\")  # Consistent naming for validation set\n",
        "test_dataset = TestAudioDataset(root, partition=\"test-clean\")   # Updated to match test dataset class\n",
        "\n",
        "# Create DataLoader objects for each dataset\n",
        "# Ensure to pass the collate function for custom batching\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config[\"batch_size\"],  # Adjustable based on system resources\n",
        "    shuffle=True,                     # Shuffle training data\n",
        "    collate_fn=train_dataset.collate_fn  # Collate function for batching\n",
        ")\n",
        "\n",
        "validation_data_loader = torch.utils.data.DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=config[\"batch_size\"],  # Adjustable based on system resources\n",
        "    shuffle=False,                    # No shuffling for validation\n",
        "    collate_fn=validation_dataset.collate_fn  # Collate function for batching\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config[\"batch_size\"],  # Adjustable based on system resources\n",
        "    shuffle=False,                    # No shuffling for test data\n",
        "    collate_fn=test_dataset.collate_fn  # Collate function for batching\n",
        ")\n",
        "\n",
        "# Print dataset and loader information\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Training samples: {}, Training batches: {}\".format(len(train_dataset), len(train_data_loader)))\n",
        "print(\"Validation samples: {}, Validation batches: {}\".format(len(validation_dataset), len(validation_data_loader)))\n",
        "print(\"Test samples: {}, Test batches: {}\".format(len(test_dataset), len(test_data_loader)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "0c488f96-855c-420d-bd80-28bb726bd7cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21, 1602, 28]) torch.Size([21, 168]) torch.Size([21]) torch.Size([21])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtCfi7-8y1my"
      },
      "source": [
        "# SeNet for Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "5VMVdlmryxAN"
      },
      "outputs": [],
      "source": [
        "class SqueezeExcitation1D(torch.nn.Module):\n",
        "    def __init__(self, input_channels, reduction=16):\n",
        "        super(SqueezeExcitation1D, self).__init__()\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dense1 = nn.Linear(input_channels, input_channels // reduction)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dense2 = nn.Linear(input_channels // reduction, input_channels)\n",
        "        self.scaling = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        batch_size, channels, _ = input_tensor.size()\n",
        "        squeeze = self.global_pool(input_tensor).view(batch_size, channels)\n",
        "        excitation = self.dense1(squeeze)\n",
        "        excitation = self.activation(excitation)\n",
        "        excitation = self.dense2(excitation)\n",
        "        excitation = self.scaling(excitation).view(batch_size, channels, 1)\n",
        "        return input_tensor * excitation\n",
        "\n",
        "\n",
        "class ResidualBlock1D(torch.nn.Module):\n",
        "    def __init__(self, input_channels, channel_filters):\n",
        "        super(ResidualBlock1D, self).__init__()\n",
        "\n",
        "        filter1, filter2 = channel_filters\n",
        "\n",
        "        self.conv_block1 = nn.Conv1d(input_channels, filter1, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm1 = nn.BatchNorm1d(filter1)\n",
        "\n",
        "        self.conv_block2 = nn.Conv1d(filter1, filter2, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm2 = nn.BatchNorm1d(filter2)\n",
        "\n",
        "        self.squeeze_excite = SqueezeExcitation1D(filter2)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        self.adjust_channels = None\n",
        "        if input_channels != filter2:\n",
        "            self.adjust_channels = nn.Conv1d(input_channels, filter2, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        shortcut = input_tensor\n",
        "\n",
        "        output = self.conv_block1(input_tensor)\n",
        "        output = self.norm1(output)\n",
        "        output = self.activation(output)\n",
        "\n",
        "        output = self.conv_block2(output)\n",
        "        output = self.norm2(output)\n",
        "\n",
        "        output = self.squeeze_excite(output)\n",
        "\n",
        "        if self.adjust_channels is not None:\n",
        "            shortcut = self.adjust_channels(shortcut)\n",
        "\n",
        "        output += shortcut\n",
        "        output = self.activation(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class ResNet34(torch.nn.Module):\n",
        "    def __init__(self, input_dim, feature_dim):\n",
        "        super(ResNet34, self).__init__()\n",
        "\n",
        "        self.input_layer = nn.Conv1d(input_dim, 192, kernel_size=7, stride=1, padding=3)\n",
        "        self.initial_norm = nn.BatchNorm1d(192)\n",
        "        self.initial_activation = nn.ReLU()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            ResidualBlock1D(192, [192, 192]),\n",
        "            ResidualBlock1D(192, [192, 192]),\n",
        "            ResidualBlock1D(192, [192, 192])\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            ResidualBlock1D(192, [384, 384]),\n",
        "            ResidualBlock1D(384, [384, 384]),\n",
        "            ResidualBlock1D(384, [384, 384]),\n",
        "            ResidualBlock1D(384, [384, 384])\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock1D(384, [768, 768]),\n",
        "            ResidualBlock1D(768, [768, 768]),\n",
        "            ResidualBlock1D(768, [768, 768]),\n",
        "            ResidualBlock1D(768, [768, 768]),\n",
        "            ResidualBlock1D(768, [768, 768]),\n",
        "            ResidualBlock1D(768, [768, 768])\n",
        "        )\n",
        "\n",
        "        self.block4 = nn.Sequential(\n",
        "            ResidualBlock1D(768, [feature_dim, feature_dim]),\n",
        "            ResidualBlock1D(feature_dim, [feature_dim, feature_dim]),\n",
        "            ResidualBlock1D(feature_dim, [feature_dim, feature_dim])\n",
        "        )\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output = self.input_layer(input_tensor)\n",
        "        output = self.initial_norm(output)\n",
        "        output = self.initial_activation(output)\n",
        "\n",
        "        output = self.block1(output)\n",
        "        output = self.block2(output)\n",
        "        output = self.block3(output)\n",
        "        output = self.block4(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class ResNet18(torch.nn.Module):\n",
        "    def __init__(self, input_dim, feature_dim):\n",
        "        super(ResNet18, self).__init__()\n",
        "\n",
        "        self.input_layer = nn.Conv1d(input_dim, 128, kernel_size=7, stride=1, padding=3)\n",
        "        self.initial_norm = nn.BatchNorm1d(128)\n",
        "        self.initial_activation = nn.ReLU()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            ResidualBlock1D(128, [128, 128]),\n",
        "            ResidualBlock1D(128, [128, 128])\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            ResidualBlock1D(128, [256, 256]),\n",
        "            ResidualBlock1D(256, [256, 256])\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            ResidualBlock1D(256, [512, 512]),\n",
        "            ResidualBlock1D(512, [512, 512])\n",
        "        )\n",
        "\n",
        "        self.block4 = nn.Sequential(\n",
        "            ResidualBlock1D(512, [feature_dim, feature_dim]),\n",
        "            ResidualBlock1D(feature_dim, [feature_dim, feature_dim])\n",
        "        )\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output = self.input_layer(input_tensor)\n",
        "        output = self.initial_norm(output)\n",
        "        output = self.initial_activation(output)\n",
        "\n",
        "        output = self.block1(output)\n",
        "        output = self.block2(output)\n",
        "        output = self.block3(output)\n",
        "        output = self.block4(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "### Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "qd4BEX_yMUzz"
      },
      "outputs": [],
      "source": [
        "# Utils for network\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "OmdyXI6KMUzz"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input?\n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "    def forward(self, x_packed):\n",
        "        x_packed, _ = self.blstm(x_packed)\n",
        "        x, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n",
        "        x, x_lens = self.trunc_reshape(x, x_lens)\n",
        "        x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        return x_packed\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens):\n",
        "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "        # TODO: Reduce lengths by the same downsampling factor\n",
        "        if x.size(1) % 2:\n",
        "          x = x[:, :-1]\n",
        "\n",
        "\n",
        "        # Reshape by concatenating every two time steps\n",
        "        batch_size, seq_len, feature_size = x.size()\n",
        "\n",
        "        x = x.view(batch_size, seq_len // 2, feature_size * 2)\n",
        "\n",
        "        # Update sequence lengths by dividing by 2\n",
        "        x_lens = x_lens // 2\n",
        "\n",
        "        return x, x_lens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ZQ75OcMUz0"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "GEzw5_xmMUz0"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, encoder_hidden_size, num_layers = 2):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "\n",
        "        self.embedding = nn.Sequential(\n",
        "            PermuteBlock(),\n",
        "            ResNet34(input_size, encoder_hidden_size),\n",
        "            PermuteBlock()\n",
        "        )\n",
        "\n",
        "        # Sequential pBLSTM layers\n",
        "        self.pBLSTMs = nn.Sequential(\n",
        "\n",
        "            pBLSTM(input_size=encoder_hidden_size,\n",
        "                   hidden_size=encoder_hidden_size),\n",
        "\n",
        "            pBLSTM(input_size=encoder_hidden_size * 4,\n",
        "                   hidden_size=encoder_hidden_size))\n",
        "\n",
        "    def forward(self, x, x_lens):\n",
        "        # Apply embedding layer\n",
        "        # Where are x and x_lens coming from? The dataloader\n",
        "        #TODO: Call the embedding layer\n",
        "        # TODO: Pack Padded Sequence\n",
        "        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "        # TODO: Pad Packed Sequence\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Pack, pass through pBLSTM layers, unpack\n",
        "        x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "        out_x_packed = self.pBLSTMs(x_packed)\n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(out_x_packed, batch_first=True)\n",
        "\n",
        "        return encoder_outputs, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg82HXa3MUz1"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "PQIRxdNTMUz1"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder module for transforming encoded feature embeddings into\n",
        "    class probabilities using an MLP followed by a softmax layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size, output_size=41):\n",
        "        \"\"\"\n",
        "        Initializes the Decoder module.\n",
        "\n",
        "        Args:\n",
        "            embed_size (int): The size of the input embeddings from the encoder.\n",
        "            output_size (int): The number of output classes. Default is 41.\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Define the multi-layer perceptron (MLP) for decoding\n",
        "        self.mlp = nn.Sequential(\n",
        "            PermuteBlock(),  # Custom block to permute dimensions (if required by input shape)\n",
        "            nn.BatchNorm1d(embed_size),  # Normalize input embeddings across the batch\n",
        "            PermuteBlock(),  # Restore the original dimension order after normalization\n",
        "            nn.Linear(embed_size, 384),  # First fully connected layer: embed_size -> 384\n",
        "            nn.ReLU(),  # Activation function to introduce non-linearity\n",
        "            nn.Linear(384, 128),  # Second fully connected layer: 384 -> 128\n",
        "            nn.ReLU(),  # Another ReLU activation for non-linearity\n",
        "            nn.Linear(128, output_size)  # Final layer: 128 -> output_size (number of classes)\n",
        "        )\n",
        "\n",
        "        # Apply log softmax activation to generate class probabilities\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Forward pass of the decoder.\n",
        "\n",
        "        Args:\n",
        "            encoder_out (torch.Tensor): Output from the encoder,\n",
        "                                        shape (batch_size, seq_len, embed_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Log-softmax probabilities of shape\n",
        "                          (batch_size, seq_len, output_size).\n",
        "        \"\"\"\n",
        "        out = self.mlp(encoder_out)  # Pass encoder output through the MLP\n",
        "        out = self.softmax(out)  # Apply log softmax for class probabilities\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "qmHf6pFiMUz1"
      },
      "outputs": [],
      "source": [
        "class ASRModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Automatic Speech Recognition (ASR) model combining augmentations,\n",
        "    an encoder, and a decoder to convert audio features into text predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, embed_size=512, output_size=len(PHONEMES)):\n",
        "        \"\"\"\n",
        "        Initializes the ASR model with augmentation, encoder, and decoder components.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): The size of the input feature vector (e.g., number of frequency bins).\n",
        "            embed_size (int): The embedding size for the encoder hidden states. Default is 512.\n",
        "            output_size (int): The number of output classes (e.g., phonemes). Default is length of PHONEMES.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Define augmentation layers for the input features (frequency and time masking)\n",
        "        self.augmentations = nn.Sequential(\n",
        "            PermuteBlock(),  # Permute the input tensor for proper masking\n",
        "            FrequencyMasking(freq_mask_param=10),  # Apply frequency masking (randomly masks parts of the frequency axis)\n",
        "            TimeMasking(time_mask_param=20),  # Apply time masking (randomly masks parts of the time axis)\n",
        "            PermuteBlock()  # Restore the input tensor's original dimension order after augmentations\n",
        "        )\n",
        "\n",
        "        # Define the encoder, which processes the input features and generates embeddings\n",
        "        # The encoder uses RNNs or similar architectures to process the sequence of input features.\n",
        "        self.encoder = Encoder(input_size, encoder_hidden_size=embed_size, num_layers=2)\n",
        "\n",
        "        # Define the decoder, which takes the encoded embeddings and generates output predictions\n",
        "        # The decoder uses MLPs or similar architectures to predict phonemes or other output classes.\n",
        "        self.decoder = Decoder(embed_size * 4, output_size)\n",
        "\n",
        "    def forward(self, x, lengths_x):\n",
        "        \"\"\"\n",
        "        Forward pass of the ASR model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input feature tensor (e.g., spectrogram) of shape\n",
        "                              (batch_size, seq_len, input_size).\n",
        "            lengths_x (torch.Tensor): Lengths of the input sequences, needed to handle variable sequence lengths.\n",
        "\n",
        "        Returns:\n",
        "            decoder_out (torch.Tensor): The output of the decoder, representing predicted phonemes.\n",
        "            encoder_lens (torch.Tensor): The lengths of the sequences output by the encoder.\n",
        "        \"\"\"\n",
        "        # Apply augmentations only during training to improve generalization\n",
        "        if self.training:\n",
        "            x = self.augmentations(x)\n",
        "\n",
        "        # Pass the input through the encoder to get the embeddings and sequence lengths\n",
        "        encoder_out, encoder_lens = self.encoder(x, lengths_x)\n",
        "\n",
        "        # Pass the encoder output through the decoder to get predictions\n",
        "        decoder_out = self.decoder(encoder_out)\n",
        "\n",
        "        return decoder_out, encoder_lens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7DMPDoMUz2"
      },
      "source": [
        "## Initialize ASR Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahfSIvCBmfMi",
        "outputId": "adc7374d-3e67-425d-c8f3-95ce4a71a966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21, 1726, 28]) torch.Size([21])\n"
          ]
        }
      ],
      "source": [
        "# print(x.shape, lx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcNisxZdtrmr",
        "outputId": "a259124d-60f5-494e-f4af-59352e81da40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchinfo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaaDsnnLMUz2",
        "outputId": "850ef582-5988-4b4c-94c7-2d197a318763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASRModel(\n",
            "  (augmentations): Sequential(\n",
            "    (0): PermuteBlock()\n",
            "    (1): FrequencyMasking()\n",
            "    (2): TimeMasking()\n",
            "    (3): PermuteBlock()\n",
            "  )\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): ResNet34(\n",
            "        (input_layer): Conv1d(28, 192, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (initial_norm): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (initial_activation): ReLU()\n",
            "        (block1): Sequential(\n",
            "          (0): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=192, out_features=12, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=12, out_features=192, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "          (1): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=192, out_features=12, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=12, out_features=192, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "          (2): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=192, out_features=12, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=12, out_features=192, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (block2): Sequential(\n",
            "          (0): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(192, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=384, out_features=24, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=24, out_features=384, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "            (adjust_channels): Conv1d(192, 384, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (1): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=384, out_features=24, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=24, out_features=384, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "          (2): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=384, out_features=24, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=24, out_features=384, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "          (3): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=384, out_features=24, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=24, out_features=384, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (block3): Sequential(\n",
            "          (0): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(384, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=768, out_features=48, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=48, out_features=768, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "            (adjust_channels): Conv1d(384, 768, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (1): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=768, out_features=48, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=48, out_features=768, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "          (2): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=768, out_features=48, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=48, out_features=768, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "          (3): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=768, out_features=48, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=48, out_features=768, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "          (4): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=768, out_features=48, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=48, out_features=768, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "          (5): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=768, out_features=48, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=48, out_features=768, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (block4): Sequential(\n",
            "          (0): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(768, 856, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(856, 856, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=856, out_features=53, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=53, out_features=856, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "            (adjust_channels): Conv1d(768, 856, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (1): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(856, 856, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(856, 856, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=856, out_features=53, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=53, out_features=856, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "          (2): ResidualBlock1D(\n",
            "            (conv_block1): Conv1d(856, 856, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm1): BatchNorm1d(856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv_block2): Conv1d(856, 856, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (norm2): BatchNorm1d(856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (squeeze_excite): SqueezeExcitation1D(\n",
            "              (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "              (dense1): Linear(in_features=856, out_features=53, bias=True)\n",
            "              (activation): ReLU()\n",
            "              (dense2): Linear(in_features=53, out_features=856, bias=True)\n",
            "              (scaling): Sigmoid()\n",
            "            )\n",
            "            (activation): ReLU()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): PermuteBlock()\n",
            "    )\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(856, 856, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "      (1): pBLSTM(\n",
            "        (blstm): LSTM(3424, 856, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): BatchNorm1d(3424, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): PermuteBlock()\n",
            "      (3): Linear(in_features=3424, out_features=384, bias=True)\n",
            "      (4): ReLU()\n",
            "      (5): Linear(in_features=384, out_features=128, bias=True)\n",
            "      (6): ReLU()\n",
            "      (7): Linear(in_features=128, out_features=41, bias=True)\n",
            "    )\n",
            "    (softmax): LogSoftmax(dim=2)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==============================================================================================================\n",
              "Layer (type:depth-idx)                                       Output Shape              Param #\n",
              "==============================================================================================================\n",
              "ASRModel                                                     [21, 400, 41]             --\n",
              "├─Encoder: 1-1                                               [21, 400, 3424]           --\n",
              "│    └─Sequential: 2-1                                       [21, 1602, 856]           --\n",
              "│    │    └─PermuteBlock: 3-1                                [21, 28, 1602]            --\n",
              "│    │    └─ResNet34: 3-2                                    [21, 856, 1602]           39,227,491\n",
              "│    │    └─PermuteBlock: 3-3                                [21, 1602, 856]           --\n",
              "│    └─Sequential: 2-2                                       [6776, 3424]              --\n",
              "│    │    └─pBLSTM: 3-4                                      [13563, 3424]             11,737,472\n",
              "│    │    └─pBLSTM: 3-5                                      [6776, 3424]              29,323,136\n",
              "├─Decoder: 1-2                                               [21, 400, 41]             --\n",
              "│    └─Sequential: 2-3                                       [21, 400, 41]             --\n",
              "│    │    └─PermuteBlock: 3-6                                [21, 3424, 400]           --\n",
              "│    │    └─BatchNorm1d: 3-7                                 [21, 3424, 400]           6,848\n",
              "│    │    └─PermuteBlock: 3-8                                [21, 400, 3424]           --\n",
              "│    │    └─Linear: 3-9                                      [21, 400, 384]            1,315,200\n",
              "│    │    └─ReLU: 3-10                                       [21, 400, 384]            --\n",
              "│    │    └─Linear: 3-11                                     [21, 400, 128]            49,280\n",
              "│    │    └─ReLU: 3-12                                       [21, 400, 128]            --\n",
              "│    │    └─Linear: 3-13                                     [21, 400, 41]             5,289\n",
              "│    └─LogSoftmax: 2-4                                       [21, 400, 41]             --\n",
              "==============================================================================================================\n",
              "Total params: 81,664,716\n",
              "Trainable params: 81,664,716\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (T): 1,227.48\n",
              "==============================================================================================================\n",
              "Input size (MB): 3.77\n",
              "Forward/backward pass size (MB): 11469.05\n",
              "Params size (MB): 326.66\n",
              "Estimated Total Size (MB): 11799.48\n",
              "=============================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "model = ASRModel(\n",
        "    input_size=28,  # MFCC feature size\n",
        "    embed_size=856,\n",
        "    output_size=len(PHONEMES)\n",
        ").to(device)\n",
        "print(model)\n",
        "summary(model, input_data=x.to(device), lengths_x=lx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config\n",
        "Initialize Loss Criterion, Optimizer, CTC Beam Decoder, Scheduler, Scaler (Mixed-Precision), etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "\n",
        "# CTC Loss: Connectionist Temporal Classification (CTC) is used for sequence-to-sequence tasks like ASR.\n",
        "# CTC Loss allows training on unaligned data by considering all possible alignments of the input and target sequences.\n",
        "# Reference: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "criterion = torch.nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n",
        "# Explanation:\n",
        "# - blank=0: The blank token represents the 'no label' token in the CTC loss computation.\n",
        "# - zero_infinity=True: Ensures that infinity values are replaced by zero, useful for sequences with different lengths.\n",
        "# - reduction='mean': Specifies that the loss should be averaged over all elements (across batch and time steps).\n",
        "\n",
        "# Optimizer: AdamW optimizer is used for weight updates during training.\n",
        "# AdamW is an improved version of Adam that decouples weight decay from the optimization steps.\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-3)\n",
        "# Explanation:\n",
        "# - model.parameters(): This specifies the parameters of the model that will be updated during training.\n",
        "# - lr=config['lr']: Learning rate, taken from the configuration (config).\n",
        "# - weight_decay=1e-3: L2 regularization applied to the weights to prevent overfitting.\n",
        "\n",
        "# CTC Beam Decoder: This decoder is used to decode the output of the model (predicted phonemes) using CTC decoding.\n",
        "# It decodes the predicted probabilities into the most likely sequence of phonemes, using beam search for more accurate results.\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder = CTCBeamDecoder(\n",
        "    labels=PHONEMES,  # The list of possible phoneme labels.\n",
        "    blank_id=0,  # The index of the blank token used in CTC loss.\n",
        "    beam_width=config['beam_width'],  # Beam search width for decoding (higher values increase search space).\n",
        "    num_processes=4,  # Number of parallel processes for beam search decoding to speed up computation.\n",
        "    log_probs_input=True  # If true, the input to the decoder is assumed to be log-probabilities.\n",
        ")\n",
        "# Explanation:\n",
        "# - The decoder uses beam search to find the most likely output sequences by maintaining the top-k candidates at each decoding step.\n",
        "# - beam_width controls the number of candidates considered during each decoding step (larger values improve accuracy but increase computation).\n",
        "\n",
        "# Learning Rate Scheduler: Cosine Annealing Warm Restarts adjusts the learning rate using a cosine annealing schedule.\n",
        "# It periodically restarts the learning rate schedule to avoid getting stuck in sharp local minima and improves training convergence.\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-5)\n",
        "# Explanation:\n",
        "# - optimizer: The optimizer whose learning rate will be adjusted.\n",
        "# - T_0=5: The number of iterations before the first restart (controls how often the learning rate restarts).\n",
        "# - T_mult=2: Multiplier for the number of iterations between restarts. After each restart, the number of iterations increases.\n",
        "# - eta_min=1e-5: Minimum learning rate value during annealing.\n",
        "\n",
        "# Mixed Precision Training: This is used to accelerate training by using half-precision floating-point numbers (float16).\n",
        "# The GradScaler helps to scale the gradients during backpropagation to prevent underflow when using lower precision.\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# Explanation:\n",
        "# - scaler: Scales the gradients in mixed-precision training. This helps prevent overflow or underflow issues with lower precision.\n",
        "# - Mixed-precision training improves performance by using 16-bit floating point operations instead of the standard 32-bit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "# Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
        "    \"\"\"\n",
        "    Decode the model's output using a beam search decoder and map the decoded indices to phonemes.\n",
        "\n",
        "    Args:\n",
        "        output (torch.Tensor): The model's output, typically of shape (batch_size, seq_length, num_classes).\n",
        "        output_lens (torch.Tensor): The lengths of the output sequences for each batch element.\n",
        "        decoder: The decoder used to decode the CTC output (e.g., CTCBeamDecoder).\n",
        "        PHONEME_MAP (list): A mapping of phoneme indices to phoneme strings (default: LABELS).\n",
        "\n",
        "    Returns:\n",
        "        pred_strings (list): A list of predicted strings (phoneme sequences) for each element in the batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use the decoder to decode the model output (e.g., using beam search).\n",
        "    # beam_results contains the top-k sequences, where each sequence has probabilities for each token.\n",
        "    # out_lens are the actual lengths of the decoded sequences for each batch element.\n",
        "    beam_results, _, _, out_lens = decoder.decode(output, seq_lens=output_lens)\n",
        "\n",
        "    # List to store the predicted strings for each element in the batch.\n",
        "    pred_strings = []\n",
        "\n",
        "    # Iterate over the batch to decode each output and map it to phonemes.\n",
        "    for i in range(output_lens.shape[0]):\n",
        "        # Extract the predicted indices for the current example (beam_results[i][0] is the top candidate).\n",
        "        # Use out_lens[i][0] to slice the sequence to the actual length of the prediction.\n",
        "        predict_indices = beam_results[i][0][:out_lens[i][0]]\n",
        "\n",
        "        # Convert the indices to a string of phonemes using the PHONEME_MAP.\n",
        "        predict_string = ''.join([PHONEME_MAP[idx] for idx in predict_indices])\n",
        "\n",
        "        # Append the decoded string to the list of predictions.\n",
        "        pred_strings.append(predict_string)\n",
        "\n",
        "    return pred_strings\n",
        "\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS):\n",
        "    \"\"\"\n",
        "    Calculate the average Levenshtein distance between the predicted sequences and the ground truth labels.\n",
        "\n",
        "    Args:\n",
        "        output (torch.Tensor): The model's output predictions (same shape as in decode_prediction).\n",
        "        label (torch.Tensor): The ground truth label sequences (batch_size, max_label_length).\n",
        "        output_lens (torch.Tensor): The lengths of the output sequences for each batch element.\n",
        "        label_lens (torch.Tensor): The lengths of the label sequences for each batch element.\n",
        "        decoder: The decoder used to decode the CTC output.\n",
        "        PHONEME_MAP (list): A mapping of phoneme indices to phoneme strings (default: LABELS).\n",
        "\n",
        "    Returns:\n",
        "        dist (float): The average Levenshtein distance over the batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the total Levenshtein distance to 0.\n",
        "    dist = 0\n",
        "\n",
        "    # Get the batch size (number of examples in the batch).\n",
        "    batch_size = label.shape[0]\n",
        "\n",
        "    # Decode the predicted sequences using the decoder.\n",
        "    pred_strings = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "\n",
        "    # Iterate over each example in the batch to calculate the Levenshtein distance.\n",
        "    for i in range(batch_size):\n",
        "        # Extract the predicted and ground truth strings for the current example.\n",
        "        pred_string = pred_strings[i]\n",
        "\n",
        "        # Extract the label sequence for the current example and convert it to a string using PHONEME_MAP.\n",
        "        label_indices = label[i][:label_lens[i]]\n",
        "        label_string = ''.join([PHONEME_MAP[idx] for idx in label_indices])\n",
        "\n",
        "        # Compute the Levenshtein distance between the predicted and label strings and accumulate it.\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    # Compute the average Levenshtein distance over the batch.\n",
        "    dist /= batch_size  # Averaging the distance for the entire batch.\n",
        "\n",
        "    return dist\n",
        "    # Uncommented `return dist` at the end of function, as it's necessary for returning the computed distance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qk9iZud1LXT"
      },
      "source": [
        "# Test Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "GnTLL-5gMBrY"
      },
      "outputs": [],
      "source": [
        "# #test code to check shapes\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# model.eval()\n",
        "# for i, data in enumerate(val_loader, 0):\n",
        "#     x, y, lx, ly = data\n",
        "#     x, y = x.to(device), y.to(device)\n",
        "#     h, lh = model(x, lx)\n",
        "#     print(h.shape)\n",
        "#     h_permuted = torch.permute(h, (1, 0, 2))\n",
        "#     print(h_permuted.shape, y.shape)\n",
        "#     loss = criterion(h_permuted, y, lh, ly)\n",
        "#     print(loss)\n",
        "\n",
        "#     print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "# WandB\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "11b55639-0252-4937-9c06-b6726a4c6f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"f9cf09dac50889564c1f8fb34fff472412382ff7\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "4s52yBOvICPZ",
        "outputId": "59f8713b-3a31-4309-ec04-b0ba1483fabd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241115_221624-6nmnpjaj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/skandv-carnegie-mellon-university/hw3p2-ablations/runs/6nmnpjaj' target=\"_blank\">Resnet-34-runn</a></strong> to <a href='https://wandb.ai/skandv-carnegie-mellon-university/hw3p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/skandv-carnegie-mellon-university/hw3p2-ablations' target=\"_blank\">https://wandb.ai/skandv-carnegie-mellon-university/hw3p2-ablations</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/skandv-carnegie-mellon-university/hw3p2-ablations/runs/6nmnpjaj' target=\"_blank\">https://wandb.ai/skandv-carnegie-mellon-university/hw3p2-ablations/runs/6nmnpjaj</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"Resnet-34-runn\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16.\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint Creation"
      ],
      "metadata": {
        "id": "sjFlbLqBfBru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "import os\n",
        "\n",
        "checkpoint_dir = '/content/checkpoint'\n",
        "\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "\n",
        "    os.makedirs(checkpoint_dir)\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = \"/content/checkpoint/last_model.pth\"\n",
        "best_model_path = \"/content/checkpoint/best_model.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "JR43E28rM9Ak",
        "outputId": "a7e1d0a0-b115-40c0-ab6c-2ad4b9548eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1/35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   3%|▎         | 44/1359 [00:58<30:00,  1.37s/it, loss=0.6220, lr=0.000698]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-1d36a4cdae8e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcurr_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-162-abb89d234fb2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Another couple things you need for FP16.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is something added just for FP16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#TODO: Please complete the training loop\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer)\n",
        "    valid_loss, valid_dist = validate_model(model, val_loader, decoder)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'valid_dist': valid_dist,\n",
        "        'valid_loss': valid_loss,\n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1-mxLd53GHL"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"/content/checkpoint/best_model.pth\")\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2moYJhTWsOG-"
      },
      "outputs": [],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = 5\n",
        "\n",
        "test_decoder = CTCBeamDecoder(\n",
        "    labels=LABELS,\n",
        "    blank_id=0,\n",
        "    beam_width=TEST_BEAM_WIDTH,\n",
        "    num_processes=4,\n",
        "    log_probs_input=True\n",
        ")\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_strings = decode_prediction(h, lh, test_decoder)\n",
        "    #TODO save the output in results array.\n",
        "\n",
        "    results.extend(prediction_strings)\n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtXSpAZE7ohm"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\"index\": range(len(results)), \"label\": results})\n",
        "\n",
        "# Save to the specified file path\n",
        "df.to_csv(\"/content/submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1sZmEIs4yIz"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c hw3p2-785-f24 -f /content/submission.csv -m \"I made it!\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10 (main, Feb 16 2023, 02:49:39) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
